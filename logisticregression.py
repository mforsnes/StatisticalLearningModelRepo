# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TsTt6zk7S1DfS_m4ns4e6tsv8dmMVr-O
"""

import numpy as np
import numpy.matlib

import matplotlib.pyplot as plt

def softmax(z):
    z -= np.max(z)
    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T
    return sm

def LogisticRegression_GD(X_train, y_train, learning_rate=0.01, eta=0.1, num_iterations=5000, epsilon=1e-4):
    num_examples, num_features = X_train.shape
    num_classes = len(np.unique(y_train))

    W = np.random.randn(num_features, num_classes)
    b = np.zeros((1, num_classes))

    y_onehot = np.zeros((num_examples, num_classes))
    y_onehot[np.arange(num_examples), y_train] = 1

    losses = []

    for i in range(num_iterations):
        z = np.dot(X_train, W) + b
        y_pred = softmax(z)

        loss = -np.sum(y_onehot * np.log(y_pred)) / num_examples + eta * np.sum(W**2) / 2
        losses.append(loss)

        if i > 0 and abs(loss - losses[-2]) < epsilon:
            break

        grad_W = np.dot(X_train.T, (y_pred - y_onehot)) / num_examples + eta * W
        grad_b = np.sum(y_pred - y_onehot, axis=0) / num_examples

        W -= learning_rate * grad_W
        b -= learning_rate * grad_b

    return W, b, losses

def compute_precision(y_true, y_pred):
    unique_labels = set(y_true)
    precision_scores = []

    for label in unique_labels:
        true_positive = sum((y_true == label) & (y_pred == label))
        false_positive = sum((y_true != label) & (y_pred == label))
        precision = true_positive / (true_positive + false_positive)
        precision_scores.append(precision)

    return sum(precision_scores) / len(precision_scores)

def softmax(z):
    z -= np.max(z)
    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T
    return sm

def LogisticRegression_GD(X_train, y_train, learning_rate=0.01, eta=0.1, num_iterations=5000, epsilon=1e-4):
    num_examples, num_features = X_train.shape
    num_classes = len(np.unique(y_train))

    W = np.random.randn(num_features, num_classes)
    b = np.zeros((1, num_classes))

    y_onehot = np.zeros((num_examples, num_classes))
    y_onehot[np.arange(num_examples), y_train] = 1

    losses = []

    for i in range(num_iterations):
        z = np.dot(X_train, W) + b
        y_pred = softmax(z)

        loss = -np.sum(y_onehot * np.log(y_pred)) / num_examples + eta * np.sum(W**2) / 2
        losses.append(loss)

        if i > 0 and abs(loss - losses[-2]) < epsilon:
            break

        grad_W = np.dot(X_train.T, (y_pred - y_onehot)) / num_examples + eta * W
        grad_b = np.sum(y_pred - y_onehot, axis=0) / num_examples

        W -= learning_rate * grad_W
        b -= learning_rate * grad_b

    return W, b, losses

def compute_accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)