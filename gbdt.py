# -*- coding: utf-8 -*-
"""GBDT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wxr7lrrRixBzXKzTCA0ogz8QPAcZoqVb
"""

from multiprocessing import Pool
from functools import partial
import numpy as np
import numba
from numba import jit

class leastsquare(object):


    def __init__(self, target):
        self.target = target
        self.g = self._g
        self.h = self._h

    def pred(self, score):
        return score.mean(axis=0)

    def _g(self, true, score):
        if score is None:
            return 2 * (self.target - true)
        else:
            return 2 * (score - true)

    def _h(self, predictions):
        return np.ones_like(self.target) * 2

class logistic(object):


  def __init__(self, n_classes):
    self.n_classes = n_classes

  def pred(self, score):

    if self.n_classes > 2:
        return np.exp(score) / np.sum(np.exp(score), axis=1, keepdims=True)
    else:
        return 1 / (1 + np.exp(-score))

  def g(self, true, score):
    pred = self.pred(score)
    return -(true - pred)

  def h(self, true, score):
    pred = self.pred(score)
    return pred * (1 - pred)

# TODO: class of GBDT
class GBDT(object):


    def __init__(self,
        n_threads = None, loss = None,
        max_depth = 5, min_sample_split = 5,
        lamda = 1, gamma = 1,
        learning_rate = 0.01, num_trees = 100):

        self.n_threads = n_threads
        self.loss = loss
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.lamda = lamda
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.num_trees = num_trees
        self.trees = []

        if loss == 'mse':
            self.loss = None

        elif loss == 'log':
            self.loss = logistic(n_classes=2)


    def train_decision_tree(self, train, residuals):
        return Tree(self.loss, n_threads=self.n_threads,
                    max_depth=self.max_depth, min_sample_split=self.min_sample_split,
                    lamda=self.lamda, gamma=self.gamma).fit(train, residuals, None)

    def fit(self, train, target):
        if self.loss is None:
            self.loss = leastsquare(target)

        if len(self.trees) == 0:
            if self.loss.__class__.__name__ == "leastsquare":
                residuals = -self.loss.g(target, np.zeros_like(target))
            else:
                residuals = -self.loss.g(target, np.zeros(len(train)))
            tree = self.train_decision_tree(train, residuals)
            self.trees.append(tree)

        for _ in range(self.num_trees - 1):
            predictions = self.predict(train)
            residuals = -self.loss.g(target, predictions)
            tree = self.train_decision_tree(train, residuals)
            self.trees.append(tree)

        return self


    def predict(self, X):
        if len(self.trees) == 0:
            if self.loss.__class__.__name__ == "leastsquare":
                return np.zeros(len(X))
            else:
                return np.zeros(len(X), dtype=int)

        predictions = np.zeros(len(X))
        for tree in self.trees:
            tree_predictions = tree.predict(X)
            predictions += self.learning_rate * tree_predictions

        if self.loss.__class__.__name__ == "logistic":
            predictions = self.loss.pred(predictions)
            predictions = np.where(predictions > 0.5, 1, 0)
        return predictions

class TreeNode(object):


    def __init__(self, X, y, depth, max_depth, min_samples_split, prediction=None, left_child=None, right_child=None):
        self.depth = depth
        self.is_leaf = depth >= max_depth or X.shape[0] <= min_samples_split
        self.prediction = prediction
        self.left_child = left_child
        self.right_child = right_child

        if not self.is_leaf:
            best_feature, best_thresh = Tree.find_best_split(X, y)

            left_idxs = X[:, best_feature] <= best_thresh
            right_idxs = X[:, best_feature] > best_thresh

            self.feature = best_feature
            self.thresh = best_thresh

            left_X, left_y = X[left_idxs], y[left_idxs]
            right_X, right_y = X[right_idxs], y[right_idxs]

            self.left = TreeNode(left_X, left_y, depth+1, max_depth, min_samples_split)
            self.right = TreeNode(right_X, right_y, depth+1, max_depth, min_samples_split)


    def forward(self, x):

        if self.is_leaf:
            return self

        else:
            return self.right.forward(x)

class Tree(object):


    def __init__(self, loss, n_threads = None,
                 max_depth = 5, min_sample_split = 5,
                 lamda = 1, gamma = 1):
        self.n_threads = n_threads
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.lamda = lamda
        self.gamma = gamma
        self.int_member = 0
        self.root = None
        self.loss = loss


    def fit(self, train, g, h):
        self.root = self.construct_tree(train, g, h, self.max_depth)
        return self


    def predict(self, test):
        predictions = np.zeros(len(test))

        for i, row in enumerate(test):
            node = self.root
            while node.left_child is not None and node.right_child is not None:
                if row[node.split_feature] <= node.split_threshold:
                    node = node.left_child
                else:
                    node = node.right_child

            predictions[i] = node.prediction

        if self.loss.__class__.__name__ == "logistic":
            predictions = self.loss.pred(predictions)
            predictions = np.where(predictions > 0.5, 1, 0)

        return predictions


    def calculate_leaf_value(self, g, h):
        if self.loss.__class__.__name__ == "leastsquare":
            if g is None or len(g) == 0:
                return 0.0
            return np.mean(g)
        else:
            if g is None or len(g) == 0 or h is None or len(h) == 0:
                return 0.0
            if np.sum(h) + self.lamda == 0:
                return 0.0
            return -np.sum(g) / (np.sum(h) + self.lamda)


    def construct_tree(self, train, g, h, depth):
        if depth >= self.max_depth or train.shape[0] <= self.min_sample_split:
            prediction = self.calculate_leaf_value(g, h)
            return TreeNode(None, None, depth, self.max_depth, self.min_sample_split, prediction=prediction)

        feature, threshold, gain = self.find_best_decision_rule(train, g, h)

        if gain <= 0:
            prediction = self.calculate_leaf_value(g, h)
            return TreeNode(None, None, depth, self.max_depth, self.min_sample_split, prediction=prediction)

        indices_left = train[:, feature] <= threshold
        indices_right = train[:, feature] > threshold
        left_train, left_g, left_h = train[indices_left], g[indices_left], h[indices_left]
        right_train, right_g, right_h = train[indices_right], g[indices_right], h[indices_right]

        left_child = self.construct_tree(left_train, left_g, left_h, depth + 1)
        right_child = self.construct_tree(right_train, right_g, right_h, depth + 1)

        return TreeNode(train, g, depth, self.max_depth, self.min_sample_split, left_child=left_child, right_child=right_child, split_feature=feature, split_threshold=threshold)


    def find_best_decision_rule(self, train, g, h):
        best_gain = -np.inf
        best_feature = None
        best_threshold = None

        def find_best_split(feature):
            threshold, gain_temp = self.find_threshold(g, h, train[:, feature])
            return feature, threshold, gain_temp

        with Pool() as pool:
            results = pool.map(find_best_split, range(train.shape[1]))

        for feature, threshold, gain_temp in results:
            if gain_temp > best_gain:
                best_gain = gain_temp
                best_feature = feature
                best_threshold = threshold

        return [best_feature, best_threshold, best_gain]


    def calculate_gain(self, g_left, h_left, g_right, h_right):
      def calc_term(g, h):
        return np.square(g) / (h + self.lamda)

      gain = 0.5 * (calc_term(g_left, h_left) + calc_term(g_right, h_right) - calc_term(g_left + g_right, h_left + h_right)) - self.gamma
      return gain

    @staticmethod
    @jit(nopython=True)
    def find_threshold(self, g, h, train):
        if train.ndim == 1:
            train = train[:, np.newaxis]

        best_gain = -np.inf
        threshold = None
        for feature in range(train.shape[1]):
            try:
                sorted_indices = np.argsort(train[:, feature])
                if len(sorted_indices) > 1:
                    for i in range(1, len(sorted_indices)):
                        threshold_temp = (train[sorted_indices[i - 1], feature] + train[sorted_indices[i], feature]) / 2
                        gain_temp = self.calculate_gain(np.sum(g[sorted_indices[:i]]), np.sum(h[sorted_indices[:i]]), np.sum(g[sorted_indices[i:]]), np.sum(h[sorted_indices[i:]]))
                        if gain_temp > best_gain:
                            best_gain = gain_temp
                            threshold = threshold_temp
            except IndexError as e:
                print(f"test--feature issue")

        return [threshold, best_gain]

def root_mean_square_error(pred, y):
    if pred.ndim == 2:
        y_one_hot = np.eye(pred.shape[1])[y.astype(int)]
        return np.sqrt(np.mean((pred - y_one_hot) ** 2))
    else:
        return np.sqrt(np.mean((pred - y) ** 2))

def accuracy(pred, y):
    return np.sum(pred == y) / len(y)

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]
X_boston = data
y_boston = target
X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.3, random_state=8)

# load data
from sklearn.datasets import fetch_openml
X_credit, y_credit = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/', as_frame=False)
y_credit = np.array(list(map(lambda x: 1 if x == 'good' else 0, y_credit)))

# train-test split
from sklearn.model_selection import train_test_split
X_train_credit, X_test_credit, y_train_credit, y_test_credit = train_test_split(X_credit, y_credit, test_size=0.3, random_state=8)

# load data
from sklearn import datasets
breast_cancer = datasets.load_breast_cancer()
X_cancer = breast_cancer.data
y_cancer = breast_cancer.target
X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(X_cancer, y_cancer, test_size=0.3, random_state=8)

rf = GBDT(loss='mse')
rf.fit(X_train_boston, y_train_boston)
y_pred_train_boston = rf.predict(X_train_boston)
y_pred_test_boston = rf.predict(X_test_boston)
rmse_train_boston = root_mean_square_error(y_pred_train_boston, y_train_boston)
rmse_test_boston = root_mean_square_error(y_pred_test_boston, y_test_boston)
print(f"Boston housing: train RMSE = {rmse_train_boston:.3f}, test RMSE = {rmse_test_boston:.3f}")


rf = GBDT(loss=logistic(n_classes=2))
rf.fit(X_train_credit, y_train_credit)
y_pred_train_credit = rf.predict(X_train_credit)
y_pred_test_credit = rf.predict(X_test_credit)
acc_train_credit = accuracy(y_pred_train_credit, y_train_credit)
acc_test_credit = accuracy(y_pred_test_credit, y_test_credit)
print(f"Credit: train accuracy = {acc_train_credit:.3f}, test accuracy = {acc_test_credit:.3f}")


rf = GBDT(loss=logistic(n_classes=2))
rf.fit(X_train_cancer, y_train_cancer)
y_pred_train_cancer = rf.predict(X_train_cancer)
y_pred_test_cancer = rf.predict(X_test_cancer)
acc_train_cancer = accuracy(y_pred_train_cancer, y_train_cancer)
acc_test_cancer = accuracy(y_pred_test_cancer, y_test_cancer)
print(f"Breast cancer: train accuracy = {acc_train_cancer:.3f}, test accuracy = {acc_test_cancer:.3f}")