# -*- coding: utf-8 -*-
"""RandomForests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cjnIxznl9WPfS_oHw9Bnw_rGVn_KPZ2F
"""

import numpy as np
import numpy.matlib

import matplotlib.pyplot as plt

class leastsquare(object):

    def __init__(self):
        pass

    def pred(self, score):
        return score.mean(axis=0)

    def g(self, true, score):
        return 2 * (score - true)

    def h(self, true, score):
        return np.ones_like(true) * 2


class logistic(object):

    def __init__(self, n_classes):
        self.n_classes = n_classes

    def pred(self, score):
        return 1 / (1 + np.exp(-score))

    def g(self, true, score):
        pred = self.pred(score)
        return -(true - pred)

    def h(self, true, score):
        pred = self.pred(score)
        return pred * (1 - pred)

class RF(object):

    def __init__(self, loss,
        n_threads = None,
        max_depth = 3, min_sample_split = 1,
        lamda = 2, gamma = 2,
        rf = 10, num_trees = 100):

        self.n_threads = n_threads
        self.loss = loss
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.lamda = lamda
        self.gamma = gamma
        self.rf = rf
        self.num_trees = num_trees
        self.trees = []
        if loss == 'mse':
            self.loss = leastsquare()

        elif loss == 'log':
            self.loss = logistic(n_classes=2)

    @staticmethod
    def generate_bootstrap_samples(X, y, num_samples):
      X_boot = []
      y_boot = []
      for _ in range(num_samples):
        indices = np.random.choice(len(X), size=len(X), replace=True)
        X_boot.append(X[indices])
        y_boot.append(y[indices])
      return X_boot, y_boot


    def fit(self, X_train, y_train):
      X_boot, y_boot = self.generate_bootstrap_samples(X_train, y_train, self.num_trees)

      for i in range(self.num_trees):
        initial_prediction = np.mean(y_boot[i])
        g = self.loss.g(y_boot[i], initial_prediction)
        h = self.loss.h(y_boot[i], initial_prediction)
        tree = Tree(loss=self.loss, n_threads=self.n_threads,
                    max_depth=self.max_depth, min_sample_split=self.min_sample_split,
                    lamda=self.lamda, gamma=self.gamma, rf=self.rf)
        tree.fit(X_boot[i], g, h)
        self.trees.append(tree)
      return self

    def predict(self, test):
        if self.loss.__class__.__name__ == "leastsquare":
            predictions = np.zeros(len(test))
        else:
            predictions = np.zeros(len(test))

        for tree in self.trees:
            tree_predictions = tree.predict(test)
            predictions += tree_predictions

        predictions /= len(self.trees)

        if self.loss.__class__.__name__ == "leastsquare":
            return predictions
        else:
            return np.where(predictions > 0.5, 1, 0)

class TreeNode(object):

    def __init__(self, X, y, depth, max_depth, min_samples_split, prediction=None, left_child=None, right_child=None):
        self.depth = depth
        self.is_leaf = depth >= max_depth or X.shape[0] <= min_samples_split
        self.prediction = prediction
        self.left_child = left_child
        self.right_child = right_child

        if not self.is_leaf:
            best_feature, best_thresh = Tree.find_best_split(X, y)

            left_idxs = X[:, best_feature] <= best_thresh
            right_idxs = X[:, best_feature] > best_thresh

            self.feature = best_feature
            self.thresh = best_thresh

            left_X, left_y = X[left_idxs], y[left_idxs]
            right_X, right_y = X[right_idxs], y[right_idxs]

            self.left = TreeNode(left_X, left_y, depth+1, max_depth, min_samples_split)
            self.right = TreeNode(right_X, right_y, depth+1, max_depth, min_samples_split)


    def forward(self, x):

        if self.is_leaf:
            return self

        else:
            return self.right.forward(x)

class Tree(object):

    def __init__(self, loss, n_threads = None,
                 max_depth = 3, min_sample_split = 1,
                 lamda = 2, gamma = 2, rf = 10):
        self.n_threads = n_threads
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.lamda = lamda
        self.gamma = gamma
        self.rf = rf
        self.int_member = 0
        self.root = None
        self.loss = loss


    def fit(self, train, g, h):
        self.root = self.construct_tree(train, g, h, self.max_depth)
        return self


    def predict(self, test):
        predictions = np.zeros(len(test))

        for i, row in enumerate(test):
            node = self.root
            while node.left_child is not None and node.right_child is not None:
                if row[node.split_feature] <= node.split_threshold:
                    node = node.left_child
                else:
                    node = node.right_child

            predictions[i] = node.prediction

        if self.loss.__class__.__name__ == "logistic":
            predictions = self.loss.pred(predictions)

        return predictions


    def calculate_leaf_value(self, g, h):
        if self.loss.__class__.__name__ == "leastsquare":
            return np.mean(g)

        else:
            return -np.sum(g) / (np.sum(h) + self.lamda)


    def construct_tree(self, train, g, h, depth):
        if depth >= self.max_depth or train.shape[0] <= self.min_sample_split:
            prediction = self.calculate_leaf_value(g, h)
            return TreeNode(None, None, depth, self.max_depth, self.min_sample_split, prediction=prediction)

        feature, threshold, gain = self.find_best_decision_rule(train, g, h)

        if gain <= 0:
            prediction = self.calculate_leaf_value(g, h)
            return TreeNode(None, None, depth, self.max_depth, self.min_sample_split, prediction=prediction)

        indices_left = train[:, feature] <= threshold
        indices_right = train[:, feature] > threshold
        left_train, left_g, left_h = train[indices_left], g[indices_left], h[indices_left]
        right_train, right_g, right_h = train[indices_right], g[indices_right], h[indices_right]

        left_child = self.construct_tree(left_train, left_g, left_h, depth + 1)
        right_child = self.construct_tree(right_train, right_g, right_h, depth + 1)

        return TreeNode(train, g, depth, self.max_depth, self.min_sample_split, left_child=left_child, right_child=right_child, split_feature=feature, split_threshold=threshold)


    def find_best_decision_rule(self, train, g, h):
        best_gain = -np.inf
        best_feature = None
        best_threshold = None

        random_features = np.random.choice(train.shape[1], size=self.rf, replace=False)

        for feature in random_features:
            threshold, gain_temp = self.find_threshold(g, h, train[:, feature])
            if gain_temp > best_gain:
                best_gain = gain_temp
                best_feature = feature
                best_threshold = threshold

        return [best_feature, best_threshold, best_gain]


    def calculate_gain(self, g_left, h_left, g_right, h_right):
      def calc_term(g, h):
        return np.square(g) / (h + self.lamda)

      gain = 0.5 * (calc_term(g_left, h_left) + calc_term(g_right, h_right) - calc_term(g_left + g_right, h_left + h_right)) - self.gamma
      return gain


    def find_threshold(self, g, h, train):
        if train.ndim == 1:
            train = train[:, np.newaxis]

        best_gain = -np.inf
        threshold = None
        for feature in range(train.shape[1]):
            try:
                sorted_indices = np.argsort(train[:, feature])

                if len(sorted_indices) > 1:
                    for i in range(1, len(sorted_indices)):
                        threshold_temp = (train[sorted_indices[i - 1], feature] + train[sorted_indices[i], feature]) / 2
                        gain_temp = self.calculate_gain(np.sum(g[sorted_indices[:i]]), np.sum(h[sorted_indices[:i]]), np.sum(g[sorted_indices[i:]]), np.sum(h[sorted_indices[i:]]))

                        if gain_temp > best_gain:
                            best_gain = gain_temp
                            threshold = threshold_temp
            except IndexError as e:
                print(f"test--feature issue")

        return [threshold, best_gain]

class Tree(object):

    def __init__(self, loss, n_threads = None,
                 max_depth = 3, min_sample_split = 1,
                 lamda = 2, gamma = 2, rf = 10):
        self.n_threads = n_threads
        self.max_depth = max_depth
        self.min_sample_split = min_sample_split
        self.lamda = lamda
        self.gamma = gamma
        self.rf = rf
        self.int_member = 0
        self.root = None
        self.loss = loss


    def fit(self, train, g, h):
        self.root = self.construct_tree(train, g, h, self.max_depth)
        return self


    def predict(self, test):
        predictions = np.zeros(len(test))

        for i, row in enumerate(test):
            node = self.root
            while node.left_child is not None and node.right_child is not None:
                if row[node.split_feature] <= node.split_threshold:
                    node = node.left_child
                else:
                    node = node.right_child

            predictions[i] = node.prediction

        if self.loss.__class__.__name__ == "logistic":
            predictions = self.loss.pred(predictions)

        return predictions


    def calculate_leaf_value(self, g, h):
        if self.loss.__class__.__name__ == "leastsquare":
            return np.mean(g)

        else:
            return -np.sum(g) / (np.sum(h) + self.lamda)


    def construct_tree(self, train, g, h, depth):
        if depth >= self.max_depth or train.shape[0] <= self.min_sample_split:
            prediction = self.calculate_leaf_value(g, h)
            return TreeNode(None, None, depth, self.max_depth, self.min_sample_split, prediction=prediction)

        feature, threshold, gain = self.find_best_decision_rule(train, g, h)

        if gain <= 0:
            prediction = self.calculate_leaf_value(g, h)
            return TreeNode(None, None, depth, self.max_depth, self.min_sample_split, prediction=prediction)

        indices_left = train[:, feature] <= threshold
        indices_right = train[:, feature] > threshold
        left_train, left_g, left_h = train[indices_left], g[indices_left], h[indices_left]
        right_train, right_g, right_h = train[indices_right], g[indices_right], h[indices_right]

        left_child = self.construct_tree(left_train, left_g, left_h, depth + 1)
        right_child = self.construct_tree(right_train, right_g, right_h, depth + 1)

        return TreeNode(train, g, depth, self.max_depth, self.min_sample_split, left_child=left_child, right_child=right_child, split_feature=feature, split_threshold=threshold)


    def find_best_decision_rule(self, train, g, h):
        best_gain = -np.inf
        best_feature = None
        best_threshold = None

        random_features = np.random.choice(train.shape[1], size=self.rf, replace=False)

        for feature in random_features:
            threshold, gain_temp = self.find_threshold(g, h, train[:, feature])
            if gain_temp > best_gain:
                best_gain = gain_temp
                best_feature = feature
                best_threshold = threshold

        return [best_feature, best_threshold, best_gain]


    def calculate_gain(self, g_left, h_left, g_right, h_right):
      def calc_term(g, h):
        return np.square(g) / (h + self.lamda)

      gain = 0.5 * (calc_term(g_left, h_left) + calc_term(g_right, h_right) - calc_term(g_left + g_right, h_left + h_right)) - self.gamma
      return gain


    def find_threshold(self, g, h, train):
        if train.ndim == 1:
            train = train[:, np.newaxis]

        best_gain = -np.inf
        threshold = None
        for feature in range(train.shape[1]):
            try:
                sorted_indices = np.argsort(train[:, feature])

                if len(sorted_indices) > 1:
                    for i in range(1, len(sorted_indices)):
                        threshold_temp = (train[sorted_indices[i - 1], feature] + train[sorted_indices[i], feature]) / 2
                        gain_temp = self.calculate_gain(np.sum(g[sorted_indices[:i]]), np.sum(h[sorted_indices[:i]]), np.sum(g[sorted_indices[i:]]), np.sum(h[sorted_indices[i:]]))

                        if gain_temp > best_gain:
                            best_gain = gain_temp
                            threshold = threshold_temp
            except IndexError as e:
                print(f"test--feature issue")

        return [threshold, best_gain]

rf = RF(loss='mse')
rf.fit(X_train_boston, y_train_boston)
y_pred_train_boston = rf.predict(X_train_boston)
y_pred_test_boston = rf.predict(X_test_boston)
rmse_train_boston = root_mean_square_error(y_pred_train_boston, y_train_boston)
rmse_test_boston = root_mean_square_error(y_pred_test_boston, y_test_boston)
print(f"Boston housing: train RMSE = {rmse_train_boston:.3f}, test RMSE = {rmse_test_boston:.3f}")